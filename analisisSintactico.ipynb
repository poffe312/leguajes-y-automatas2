{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis sintáctico\n",
    "\n",
    "El **análisis sintáctico** es, en el campo de la lingüística, el análisis de las funciones sintácticas o relaciones de concordancia y jerarquía que guardan las palabras agrupándose entre sí en sintagmas u oraciones.\n",
    "\n",
    "Un analizador sintáctico (o **parser**) es un programa informático que analiza una cadena de símbolos de acuerdo a las reglas de una gramática formal. Usualmente hace parte de un compilador, en cuyo caso, transforma una entrada en un **árbol sintáctico de derivación**.\n",
    "\n",
    "El análisis sintáctico convierte el texto de entrada en otras estructuras (comúnmente árboles), que son más útiles para el posterior análisis y capturan la jerarquía implícita de la entrada. Un **analizador léxico** crea **tokens** de una secuencia de caracteres de entrada y son estos tokens los que son procesados por el analizador sintáctico para construir la estructura de datos, por ejemplo un árbol de análisis o árboles de sintaxis abstracta.\n",
    "\n",
    "**Lenguajes de programación**\n",
    "El uso más común de los analizadores sintácticos es como parte de la fase de análisis de los compiladores. De modo que tienen que analizar el código fuente del lenguaje. Los lenguajes de programación tienden a basarse en gramáticas libres de contexto, debido a que se pueden escribir analizadores rápidos y eficientes para estas.\n",
    "\n",
    "Las gramáticas libres de contexto tienen una expresividad limitada y sólo pueden expresar un conjunto limitado de lenguajes. Informalmente la razón de esto es que la memoria de un lenguaje de este tipo es limitada, la gramática no puede recordar la presencia de una construcción en una entrada arbitrariamente larga y esto es necesario en un lenguaje en el que por ejemplo una variable debe ser declarada antes de que pueda ser referenciada. Las gramáticas más complejas no pueden ser analizadas de forma eficiente. Por estas razones es común crear un analizador permisivo para una gramática libre de contexto que acepta un superconjunto del lenguaje (acepta algunas construcciones inválidas), después del análisis inicial las construcciones incorrectas pueden ser filtradas.\n",
    "\n",
    "Normalmente es fácil definir una gramática libre de contexto que acepte todas las construcciones de un lenguaje pero por el contrario es prácticamente imposible construir una gramática libre de contexto que admita solo las construcciones deseadas. En cualquier caso la mayoría de analizadores no son construidos a mano sino usando generadores automáticos.\n",
    "\n",
    "**Clasificación**\n",
    "La tarea esencial de un analizador es determinar si una determinada entrada puede ser derivada desde el símbolo inicial, usando las reglas de una gramática formal, y como hacer esto, existen esencialmente dos formas:\n",
    "\n",
    "* Analizador sintáctico descendente (Top-Down-Parser): ..un analizador puede empezar con el símbolo inicial e intentar transformarlo en la entrada, intuitivamente esto sería ir dividiendo la entrada progresivamente en partes cada vez más pequeñas, de esta forma funcionan los analizadores LL, un ejemplo es el javaCC. Una mejora en estos parsers se puede logar usando GLR (Generalized Left-to-right Rightmost derivation).\n",
    "\n",
    "* Analizador sintáctico ascendente (Bottom-Up-Parser): un analizador puede empezar con la entrada e intentar llegar hasta el símbolo inicial, intuitivamente el analizador intenta encontrar los símbolos más pequeños y progresivamente construir la jerarquía de símbolos hasta el inicial, los analizadores LR funcionan así y un ejemplo es el Yacc. También existen SLR (Simple LR) o los LALR (Look-ahead LR) como también de los GLL7​ (Generalized Left-to-right Leftmost derivation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analizar HTML con Python: HTMLParser\n",
    "El módulo **html.parse** define la clase **HTMLParser** la cual sirve de base para el análisis de archivos de texto con formato en HTML y XHTML. Cuando se vincula un archivo HTML a un objeto HTMLParser, este lo procesará de principio a fin, encontrando las etiquetas de apertura, etiquetas de cierre, datos de texto y otros componentes en el archivo fuente y “procesar” cada uno de estos elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "\n",
      "<html>\n",
      "\n",
      "<head>\n",
      "\n",
      "\t<title>Primer PÃ¡gina</title>\n",
      "\n",
      "</head>\n",
      "\n",
      "<body>\n",
      "\n",
      "\t<h1> hola mundo </h1>\n",
      "\n",
      "</body>\n",
      "\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "archivo = open('holamundo.html',\"r\")\n",
    " \n",
    "for lineas in archivo.readlines() :\n",
    "    print(lineas)\n",
    " \n",
    "archivo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='holamundo.html' mode='r' encoding='cp1252'>\n"
     ]
    }
   ],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "with open('holamundo.html','r') as file:\n",
    "    parser = HTMLParser()\n",
    "    parser.feed(file.read())\n",
    "    \n",
    "print(file)    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encountered some data  : \n",
      "\n",
      "Encountered a start tag: html\n",
      "Encountered some data  : \n",
      "\n",
      "Encountered a start tag: head\n",
      "Encountered some data  : \n",
      "\t\n",
      "Encountered a start tag: title\n",
      "Encountered some data  : Primer PÃ¡gina\n",
      "Encountered an end tag : title\n",
      "Encountered some data  : \n",
      "\n",
      "Encountered an end tag : head\n",
      "Encountered some data  : \n",
      "\n",
      "Encountered a start tag: body\n",
      "Encountered some data  : \n",
      "\t\n",
      "Encountered a start tag: h1\n",
      "Encountered some data  :  hola mundo \n",
      "Encountered an end tag : h1\n",
      "Encountered some data  : \n",
      "\n",
      "Encountered an end tag : body\n",
      "Encountered some data  : \n",
      "\n",
      "Encountered an end tag : html\n"
     ]
    }
   ],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "# create a subclass and override the handler methods\n",
    "class MyHTMLParser(HTMLParser):\n",
    "   \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        print(\"Encountered a start tag:\", tag)\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        print(\"Encountered an end tag :\", tag)\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        print(\"Encountered some data  :\", data)\n",
    "\n",
    "# instantiate the parser and fed it some HTML\n",
    "parser = MyHTMLParser()\n",
    "\n",
    "with open('holamundo.html', \"r\") as f:\n",
    "    page = f.read()\n",
    "    \n",
    "    \n",
    "parser.feed(str(page))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itcolima.edu.mx 73\n",
      "twitter.com 1\n",
      "www.premiosantander.com 1\n",
      "citt.itsm.edu.mx 1\n",
      "consultaplaneacion.tecnm.mx 1\n",
      "conacytprensa.mx 1\n",
      "www.conricyt.mx 1\n",
      "sites.google.com 1\n",
      "www.facebook.com 1\n",
      "dspace.itcolima.edu.mx 1\n",
      "www.youtube.com 1\n",
      "www.universia.net.mx 1\n"
     ]
    }
   ],
   "source": [
    "# Script para abrir un archivo html e identificar las etiquetas que contiene\n",
    "\n",
    "from requests_html import HTMLSession\n",
    "from collections import Counter\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "session = HTMLSession()\n",
    "r = session.get(\"https://itcolima.edu.mx/www/\")\n",
    "unique_netlocs = Counter(urlparse(link).netloc for link in r.html.absolute_links)\n",
    "for link in unique_netlocs:\n",
    "    print(link, unique_netlocs[link])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiqueta encontrada:  DOCTYPE html\n",
      "Encountered some data  : \n",
      "\n",
      "Encountered a start tag: html\n",
      "Encountered some data  : \n",
      "\n",
      "Encountered a start tag: head\n",
      "Encountered some data  : \n",
      "\t\n",
      "Encountered a start tag: title\n",
      "Encountered some data  :  Primer PÃ¡gina \n",
      "Encountered an end tag : title\n",
      "Encountered some data  : \n",
      "\n",
      "Encountered an end tag : head\n",
      "Encountered some data  : \n",
      "\n",
      "Encountered a start tag: body\n",
      "Encountered some data  : \n",
      "\t\n",
      "Encountered a start tag: h1\n",
      "Encountered some data  :  hola mundo \n",
      "Encountered an end tag : h1\n",
      "Encountered some data  : \n",
      "\n",
      "Encountered an end tag : body\n",
      "Encountered some data  : \n",
      "\n",
      "Encountered an end tag : html\n",
      "Encountered some data  : \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Script para identificar si el archivo html contiene la etiqueta !DOCTYPE html\n",
    "\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "# create a subclass and override the handler methods\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    \n",
    "    def handle_decl(self, data):\n",
    "        print(\"Etiqueta encontrada: \", data)\n",
    "   \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        print(\"Encountered a start tag:\", tag)\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        print(\"Encountered an end tag :\", tag)\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        print(\"Encountered some data  :\", data)\n",
    "\n",
    "# instantiate the parser and fed it some HTML\n",
    "parser = MyHTMLParser()\n",
    "\n",
    "with open('holamundo.html', \"r\") as f:\n",
    "    page = f.read()\n",
    "    \n",
    "    \n",
    "parser.feed(str(page))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrucción de un analizador sintáctico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Analizador Léxico de la sentencia print --\n",
      "-- Reconoce: print(expresion); --\n",
      "Introduce sentencia print: print(10+5);\n",
      "Token('PRINT', 'print')\n",
      "Token('OPEN_PAREN', '(')\n",
      "Token('NUMBER', '10')\n",
      "Token('SUM', '+')\n",
      "Token('NUMBER', '5')\n",
      "Token('CLOSE_PAREN', ')')\n",
      "Token('SEMI_COLON', ';')\n"
     ]
    }
   ],
   "source": [
    "from rply import LexerGenerator\n",
    "#from lexer import Lexer\n",
    "\n",
    "\n",
    "class Lexer():\n",
    "    def __init__(self):\n",
    "        self.lexer = LexerGenerator()\n",
    "\n",
    "    def _add_tokens(self):\n",
    "        # Print\n",
    "        self.lexer.add('PRINT', r'print')\n",
    "        # Parenthesis\n",
    "        self.lexer.add('OPEN_PAREN', r'\\(')\n",
    "        self.lexer.add('CLOSE_PAREN', r'\\)')\n",
    "        # Semi Colon\n",
    "        self.lexer.add('SEMI_COLON', r'\\;')\n",
    "        # Operators\n",
    "        self.lexer.add('SUM', r'\\+')\n",
    "        self.lexer.add('SUB', r'\\-')\n",
    "        self.lexer.add('MUL', r'\\*')\n",
    "        # Number\n",
    "        self.lexer.add('NUMBER', r'\\d+')\n",
    "        # Ignore spaces\n",
    "        self.lexer.ignore('\\s+')\n",
    "\n",
    "    def get_lexer(self):\n",
    "        self._add_tokens()\n",
    "        return self.lexer.build()\n",
    "    \n",
    "print(\"-- Analizador Léxico de la sentencia print --\")\n",
    "print(\"-- Reconoce: print(expresion); --\")\n",
    "\n",
    "text_input = input(\"Introduce sentencia print: \")\n",
    "\n",
    "#text_input = \"\"\"\n",
    "#print(10 + 15 - 2 + 23 * 2);\n",
    "#\"\"\"\n",
    "\n",
    "lexer = Lexer().get_lexer()\n",
    "tokens = lexer.lex(text_input)\n",
    "\n",
    "for token in tokens:\n",
    "    print(token)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Analizador Léxico de la sentencia print --\n",
      "-- Reconoce: print(expresion); --\n",
      "Introduce sentencia print: print(2+2*5);\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\f3r_c\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:81: ParserGeneratorWarning: 16 shift/reduce conflicts\n"
     ]
    }
   ],
   "source": [
    "from rply import ParserGenerator\n",
    "\n",
    "class Number():\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def eval(self):\n",
    "        return int(self.value)\n",
    "\n",
    "\n",
    "class BinaryOp():\n",
    "    def __init__(self, left, right):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "\n",
    "class Sum(BinaryOp):\n",
    "    def eval(self):\n",
    "        return self.left.eval() + self.right.eval()\n",
    "\n",
    "\n",
    "class Sub(BinaryOp):\n",
    "    def eval(self):\n",
    "        return self.left.eval() - self.right.eval()\n",
    "\n",
    "class Mul(BinaryOp):\n",
    "    def eval(self):\n",
    "        return self.left.eval() * self.right.eval()\n",
    "    \n",
    "class Div(BinaryOp):\n",
    "    def eval(self):\n",
    "        return self.left.eval() / self.right.eval()\n",
    "    \n",
    "class Print():\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def eval(self):\n",
    "        print(self.value.eval())\n",
    "        \n",
    "        \n",
    "class Parser():\n",
    "    def __init__(self):\n",
    "        self.pg = ParserGenerator(\n",
    "            # A list of all token names accepted by the parser.\n",
    "            ['NUMBER', 'PRINT', 'OPEN_PAREN', 'CLOSE_PAREN',\n",
    "             'SEMI_COLON', 'SUM', 'SUB', 'DIV', 'MUL']\n",
    "        )\n",
    "\n",
    "    def parse(self):\n",
    "        @self.pg.production('program : PRINT OPEN_PAREN expression CLOSE_PAREN SEMI_COLON')\n",
    "        def program(p):\n",
    "            return Print(p[2])\n",
    "\n",
    "        @self.pg.production('expression : expression SUM expression')\n",
    "        @self.pg.production('expression : expression SUB expression')\n",
    "        @self.pg.production('expression : expression MUL expression')\n",
    "        @self.pg.production('expression : expression DIV expression')\n",
    "        def expression(p):\n",
    "            left = p[0]\n",
    "            right = p[2]\n",
    "            operator = p[1]\n",
    "            if operator.gettokentype() == 'SUM':\n",
    "                return Sum(left, right)\n",
    "            elif operator.gettokentype() == 'SUB':\n",
    "                return Sub(left, right)\n",
    "            elif operator.gettokentype() == 'MUL':\n",
    "                return Mul(left, right)\n",
    "            elif operator.gettokentype() == 'DIV':\n",
    "                return Div(left, right)\n",
    "\n",
    "        @self.pg.production('expression : NUMBER')\n",
    "        def number(p):\n",
    "            return Number(p[0].value)\n",
    "\n",
    "        @self.pg.error\n",
    "        def error_handle(token):\n",
    "            raise ValueError(token)\n",
    "\n",
    "    def get_parser(self):\n",
    "        return self.pg.build()\n",
    "    \n",
    "print(\"-- Analizador Léxico de la sentencia print --\")\n",
    "print(\"-- Reconoce: print(expresion); --\")\n",
    "\n",
    "text_input = input(\"Introduce sentencia print: \")\n",
    "\n",
    "lexer = Lexer().get_lexer()\n",
    "tokens = lexer.lex(text_input)\n",
    "\n",
    "pg = Parser()\n",
    "pg.parse()\n",
    "parser = pg.get_parser()\n",
    "parser.parse(tokens).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
